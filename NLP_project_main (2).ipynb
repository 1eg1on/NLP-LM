{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_project_main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42uHTteFMY-p",
        "outputId": "ba6aac45-d3ee-48ee-83a3-9dcfca32bf33"
      },
      "source": [
        "import os\r\n",
        "\r\n",
        "# cloning the directory to work in\r\n",
        "\r\n",
        "!git clone https://github.com/1eg1on/nlp-lm\r\n",
        "os.chdir(os.getcwd())\r\n",
        "\r\n",
        "\r\n",
        "# installing the requirements\r\n",
        "\r\n",
        "!pip install -r requirements.txt\r\n",
        "\r\n",
        "# creating the data-folder and downoading the main body of the dataset\r\n",
        "\r\n",
        "!mkdir data\r\n",
        "!mkdir data/fever-data\r\n",
        "\r\n",
        "# We use the data used in the baseline paper\r\n",
        "!wget -O data/fever-data/train.jsonl https://s3-eu-west-1.amazonaws.com/fever.public/train.jsonl\r\n",
        "!wget -O data/fever-data/dev.jsonl https://s3-eu-west-1.amazonaws.com/fever.public/paper_dev.jsonl\r\n",
        "!wget -O data/fever-data/test.jsonl https://s3-eu-west-1.amazonaws.com/fever.public/paper_test.jsonl"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'nlp-lm' already exists and is not an empty directory.\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (0.22.2.post1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (4.41.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (3.2.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->-r requirements.txt (line 3)) (0.17.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->-r requirements.txt (line 5)) (1.15.0)\n",
            "mkdir: cannot create directory ‘data’: File exists\n",
            "mkdir: cannot create directory ‘data/fever-data’: File exists\n",
            "--2020-12-14 13:59:52--  https://s3-eu-west-1.amazonaws.com/fever.public/train.jsonl\n",
            "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.61.75\n",
            "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.61.75|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 33024303 (31M) [application/x-www-form-urlencoded]\n",
            "Saving to: ‘data/fever-data/train.jsonl’\n",
            "\n",
            "data/fever-data/tra 100%[===================>]  31.49M  21.5MB/s    in 1.5s    \n",
            "\n",
            "2020-12-14 13:59:54 (21.5 MB/s) - ‘data/fever-data/train.jsonl’ saved [33024303/33024303]\n",
            "\n",
            "--2020-12-14 13:59:54--  https://s3-eu-west-1.amazonaws.com/fever.public/paper_dev.jsonl\n",
            "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.61.75\n",
            "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.61.75|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2168767 (2.1M) [binary/octet-stream]\n",
            "Saving to: ‘data/fever-data/dev.jsonl’\n",
            "\n",
            "data/fever-data/dev 100%[===================>]   2.07M  3.41MB/s    in 0.6s    \n",
            "\n",
            "2020-12-14 13:59:55 (3.41 MB/s) - ‘data/fever-data/dev.jsonl’ saved [2168767/2168767]\n",
            "\n",
            "--2020-12-14 13:59:55--  https://s3-eu-west-1.amazonaws.com/fever.public/paper_test.jsonl\n",
            "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.61.75\n",
            "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.61.75|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2181168 (2.1M) [binary/octet-stream]\n",
            "Saving to: ‘data/fever-data/test.jsonl’\n",
            "\n",
            "data/fever-data/tes 100%[===================>]   2.08M  3.44MB/s    in 0.6s    \n",
            "\n",
            "2020-12-14 13:59:56 (3.44 MB/s) - ‘data/fever-data/test.jsonl’ saved [2181168/2181168]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxYbpJnSB_sz"
      },
      "source": [
        "# Новый раздел"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh77ORkOdrxl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58517dca-fa27-4903-dcf4-06b9cd129f9a"
      },
      "source": [
        "# Obtain the Wikipedia data\r\n",
        "\r\n",
        "!wget https://s3-eu-west-1.amazonaws.com/fever.public/wiki-pages.zip\r\n",
        "!unzip wiki-pages.zip -d data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-14 14:00:06--  https://s3-eu-west-1.amazonaws.com/fever.public/wiki-pages.zip\n",
            "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.80.146\n",
            "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.80.146|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1713485474 (1.6G) [application/zip]\n",
            "Saving to: ‘wiki-pages.zip’\n",
            "\n",
            "wiki-pages.zip       56%[==========>         ] 915.58M  34.0MB/s    eta 22s    "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqK9ZpHoeIRk"
      },
      "source": [
        "# we use the pre-written script to create a database we shortly will be working with\r\n",
        "\r\n",
        "# Need to pre-install stopwords module in case in is not done yet, used in the script\r\n",
        "\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "\r\n",
        "!python build_db.py data/wiki-pages data/single --num-files 1\r\n",
        "!python build_db.py data/wiki-pages data/fever --num-files 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MegYx0waiHYh",
        "outputId": "71f1f76b-8394-4305-c006-0287502a420d"
      },
      "source": [
        "# Building the matrixes to work with further\r\n",
        "nltk.download('punkt')\r\n",
        "\r\n",
        "!python build_count_matrix.py data/fever data/index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "INFO:Build count matrix:Processing file 0...\n",
            "INFO:Build count matrix:Counting words...\n",
            "INFO:Build count matrix:Mapping...\n",
            "INFO:Build count matrix:-------------------------Batch 1/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 2/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 3/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 4/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 5/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 6/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 7/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 8/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 9/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 10/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 11/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 12/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 13/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 14/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 15/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 16/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 17/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 18/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 19/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 20/20-------------------------\n",
            "INFO:Build count matrix:Creating sparse matrix...\n",
            "INFO:Build count matrix:Getting word-doc frequencies...\n",
            "INFO:Build count matrix:Creating data directory\n",
            "INFO:Build count matrix:Saving to data/index/fever2-ngram=1-hash=16777216.npz\n",
            "INFO:Build count matrix:Processing file 1...\n",
            "INFO:Build count matrix:Counting words...\n",
            "INFO:Build count matrix:Mapping...\n",
            "INFO:Build count matrix:-------------------------Batch 1/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 2/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 3/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 4/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 5/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 6/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 7/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 8/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 9/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 10/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 11/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 12/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 13/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 14/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 15/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 16/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 17/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 18/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 19/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 20/20-------------------------\n",
            "INFO:Build count matrix:Creating sparse matrix...\n",
            "INFO:Build count matrix:Getting word-doc frequencies...\n",
            "INFO:Build count matrix:Saving to data/index/fever0-ngram=1-hash=16777216.npz\n",
            "INFO:Build count matrix:Processing file 2...\n",
            "INFO:Build count matrix:Counting words...\n",
            "INFO:Build count matrix:Mapping...\n",
            "INFO:Build count matrix:-------------------------Batch 1/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 2/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 3/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 4/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 5/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 6/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 7/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 8/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 9/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 10/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 11/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 12/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 13/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 14/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 15/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 16/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 17/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 18/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 19/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 20/20-------------------------\n",
            "INFO:Build count matrix:Creating sparse matrix...\n",
            "INFO:Build count matrix:Getting word-doc frequencies...\n",
            "INFO:Build count matrix:Saving to data/index/fever1-ngram=1-hash=16777216.npz\n",
            "INFO:Build count matrix:Processing file 3...\n",
            "INFO:Build count matrix:Counting words...\n",
            "INFO:Build count matrix:Mapping...\n",
            "INFO:Build count matrix:-------------------------Batch 1/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 2/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 3/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 4/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 5/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 6/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 7/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 8/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 9/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 10/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 11/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 12/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 13/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 14/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 15/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 16/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 17/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 18/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 19/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 20/20-------------------------\n",
            "INFO:Build count matrix:Creating sparse matrix...\n",
            "INFO:Build count matrix:Getting word-doc frequencies...\n",
            "INFO:Build count matrix:Saving to data/index/fever3-ngram=1-hash=16777216.npz\n",
            "INFO:Build count matrix:Processing file 4...\n",
            "INFO:Build count matrix:Counting words...\n",
            "INFO:Build count matrix:Mapping...\n",
            "INFO:Build count matrix:-------------------------Batch 1/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 2/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 3/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 4/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 5/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 6/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 7/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 8/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 9/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 10/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 11/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 12/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 13/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 14/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 15/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 16/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 17/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 18/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 19/20-------------------------\n",
            "INFO:Build count matrix:-------------------------Batch 20/20-------------------------\n",
            "INFO:Build count matrix:Creating sparse matrix...\n",
            "INFO:Build count matrix:Getting word-doc frequencies...\n",
            "INFO:Build count matrix:Saving to data/index/fever4-ngram=1-hash=16777216.npz\n",
            "INFO:Merge count matrix:Loading the zeroth count matrix...\n",
            "Traceback (most recent call last):\n",
            "  File \"merge_count_matrix.py\", line 32, in <module>\n",
            "    mat, metadata = utils.load_sparse_csr(ct_files[0])\n",
            "  File \"/content/fever/utils.py\", line 56, in load_sparse_csr\n",
            "    return matrix, loader['metadata'].item(0) if 'metadata' in loader else None\n",
            "  File \"/usr/lib/python3.6/_collections_abc.py\", line 666, in __contains__\n",
            "    self[key]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\", line 262, in __getitem__\n",
            "    pickle_kwargs=self.pickle_kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/lib/format.py\", line 739, in read_array\n",
            "    raise ValueError(\"Object arrays cannot be loaded when \"\n",
            "ValueError: Object arrays cannot be loaded when allow_pickle=False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwdtK840Hm1N",
        "outputId": "0a19e0ce-3f80-4382-b05e-12770acd4ecc"
      },
      "source": [
        "!python merge_count_matrix.py data/index data/index"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:Merge count matrix:Loading the zeroth count matrix...\n",
            "INFO:Merge count matrix:Loading 1th count matrix...\n",
            "INFO:Merge count matrix:Merging...\n",
            "INFO:Merge count matrix:Finished merging\n",
            "INFO:Merge count matrix:Loading 2th count matrix...\n",
            "INFO:Merge count matrix:Merging...\n",
            "INFO:Merge count matrix:Finished merging\n",
            "INFO:Merge count matrix:Loading 3th count matrix...\n",
            "INFO:Merge count matrix:Merging...\n",
            "tcmalloc: large alloc 1353056256 bytes == 0xadc30000 @  0x7fe5120d41e7 0x7fe50fc7a5e1 0x7fe50fcdec78 0x7fe50fcdef37 0x7fe50fd76f28 0x50a4a5 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x50ad03 0x634e72 0x634f27 0x6386df 0x639281 0x4b0dc0 0x7fe511cd1bf7 0x5b259a\n",
            "INFO:Merge count matrix:Finished merging\n",
            "INFO:Merge count matrix:Loading 4th count matrix...\n",
            "INFO:Merge count matrix:Merging...\n",
            "tcmalloc: large alloc 1702707200 bytes == 0xfe690000 @  0x7fe5120d41e7 0x7fe50fc7a5e1 0x7fe50fcdec78 0x7fe50fcdef37 0x7fe50fd76f28 0x50a4a5 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x50ad03 0x634e72 0x634f27 0x6386df 0x639281 0x4b0dc0 0x7fe511cd1bf7 0x5b259a\n",
            "INFO:Merge count matrix:Finished merging\n",
            "INFO:Merge count matrix:Saving to data/index/count-ngram=1-hash=16777216.npz\n",
            "tcmalloc: large alloc 1702707200 bytes == 0x3fb7c000 @  0x7fe5120d41e7 0x7fe50fc7a5e1 0x7fe50fce1420 0x7fe50fd76df7 0x566f73 0x59fd0e 0x7fe50fcca4ed 0x50a12f 0x50beb4 0x507be4 0x509900 0x50a2fd 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x50ad03 0x634e72 0x634f27 0x6386df 0x639281 0x4b0dc0 0x7fe511cd1bf7 0x5b259a\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGyfdB4me-S3",
        "outputId": "e6bdb6ad-29bb-46fe-ba54-4bad61d52c72"
      },
      "source": [
        "# testing the TF-IDF approach further, reconstructing the matrix using the pre-written script\r\n",
        "\r\n",
        "!python reweight_count_matrix.py data/index/count-ngram\\=1-hash\\=16777216.npz data/index --model tfidf"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:Reweight count matrix:Loading count matrix...\n",
            "tcmalloc: large alloc 1702707200 bytes == 0x4e64000 @  0x7fd15c2861e7 0x7fd159dec5e1 0x7fd159e50c78 0x7fd159defe61 0x551555 0x5a9dac 0x50a433 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96 0x508cd5 0x594a01 0x59fd0e 0x5576d8 0x50c19e 0x5095c8 0x50a2fd 0x50beb4 0x507be4 0x50ad03 0x634e72 0x634f27 0x6386df 0x639281 0x4b0dc0 0x7fd15be83bf7 0x5b259a\n",
            "INFO:Reweight count matrix:Making tfidf vectors...\n",
            "tcmalloc: large alloc 1702707200 bytes == 0x1009c0000 @  0x7fd15c2861e7 0x7fd159dec5e1 0x7fd159e53420 0x7fd159ee0f87 0x50a4a5 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4 0x509900 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x507be4 0x50ad03 0x634e72 0x634f27 0x6386df 0x639281 0x4b0dc0 0x7fd15be83bf7 0x5b259a\n",
            "tcmalloc: large alloc 1702707200 bytes == 0xd92da000 @  0x7fd15c2861e7 0x7fd159dec5e1 0x7fd159e50c78 0x7fd159e50d93 0x7fd159f03fed 0x7fd159f0494e 0x7fd159f072c8 0x7fd15a047276 0x7fd15a048d44 0x7fd15a04b492 0x7fd15a04c30e 0x5a9dac 0x50a433 0x50beb4 0x507be4 0x509900 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x507be4 0x50ad03 0x634e72 0x634f27 0x6386df 0x639281 0x4b0dc0 0x7fd15be83bf7 0x5b259a\n",
            "tcmalloc: large alloc 1702707200 bytes == 0x1a169a000 @  0x7fd15c2861e7 0x7fd159dec5e1 0x7fd159e53420 0x7fd159e53682 0x7fd159e53b3e 0x7fd159e54395 0x7fd159eeb65d 0x50a4a5 0x50cc96 0x507be4 0x509900 0x50a2fd 0x50cc96 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x508cd5 0x594a01 0x59fd0e 0x4b1eea 0x619c41 0x5a0f85 0x50d064 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd\n",
            "INFO:Reweight count matrix:Saving to data/index/tfidf-count-ngram=1-hash=16777216.npz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JyO4v9GG8ke"
      },
      "source": [
        "import numpy as np\r\n",
        "import scipy.sparse as sp\r\n",
        "import pandas as pd\r\n",
        "import os\r\n",
        "import json\r\n",
        "\r\n",
        "from tqdm import tqdm\r\n",
        "from collections import Counter\r\n",
        "from itertools import product\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "\r\n",
        "import fever\r\n",
        "import utils"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eaZxQfUKUaI"
      },
      "source": [
        "DB_PATH = 'data/single/fever.db'\r\n",
        "MAT_PATH = 'data/index/tfidf-count-ngram=1-hash=16777216.npz'"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "S5-pP3inM679",
        "outputId": "73533c7b-7306-43d4-c004-7580917c7f69"
      },
      "source": [
        "oracle = fever.Oracle()"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-87-6e8d9cbeddb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moracle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfever\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOracle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/fever/fever.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, db_path, mat_path)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDocDB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdb_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_sparse_csr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# doc_freqs, hash_size, ngram, doc_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/fever/utils.py\u001b[0m in \u001b[0;36mload_sparse_csr\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mnp_load_old\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp_load_old\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/_collections_abc.py\u001b[0m in \u001b[0;36m__contains__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    664\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 666\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    667\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    260\u001b[0m                 return format.read_array(bytes,\n\u001b[1;32m    261\u001b[0m                                          \u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_pickle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                                          pickle_kwargs=self.pickle_kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0;31m# The array contained Python objects. We need to unpickle the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m             raise ValueError(\"Object arrays cannot be loaded when \"\n\u001b[0m\u001b[1;32m    740\u001b[0m                              \"allow_pickle=False\")\n\u001b[1;32m    741\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpickle_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Object arrays cannot be loaded when allow_pickle=False"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCuYEp7XM7De"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56U6_qpXM7IA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lB4jx9MhM7LY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}